# Story 2.3: Create End-to-End Pipeline Orchestrator

## Status
Ready for Review

## Story
**As a** System Administrator,
**I want** a main pipeline orchestrator that coordinates data ingestion from all APIs, transformation, attribution, and BigQuery insertion,
**so that** we can run complete daily data processing jobs reliably with proper error handling and monitoring.

## Acceptance Criteria
1. Pipeline orchestrator coordinates data ingestion from Cursor API, Anthropic API, and Google Sheets
2. Implements proper error handling and retry logic for each pipeline stage
3. Includes comprehensive metrics collection and health check reporting
4. Supports both development and production execution modes
5. Provides detailed logging with request_id traceability throughout pipeline
6. Handles partial failures gracefully (continue processing other data sources if one fails)

## Tasks / Subtasks
- [x] Create main orchestrator class (AC: 1)
  - [x] Build DailyJobOrchestrator in `src/orchestration/daily_job.py`
  - [x] Initialize all API clients (Cursor, Anthropic, GoogleSheets)
  - [x] Setup multi-platform transformer and attribution engine
  - [x] Configure BigQuery storage client for data insertion

- [x] Implement data ingestion coordination (AC: 1, 6)
  - [x] Fetch data from Cursor API with error handling
  - [x] Fetch data from Anthropic API (usage + cost) with error handling
  - [x] Fetch API key mappings from Google Sheets
  - [x] Continue processing if one API fails (partial success model)

- [x] Build transformation and attribution pipeline (AC: 1)
  - [x] Transform multi-platform data using existing transformers
  - [x] Apply user attribution using Google Sheets mappings
  - [x] Generate BigQuery-ready row format
  - [x] Validate data quality throughout pipeline

- [x] Implement BigQuery data insertion (AC: 1)
  - [x] Insert usage data to fct_usage_daily table
  - [x] Insert cost data to fct_cost_daily table
  - [x] Use 1000-record batch optimization
  - [x] Handle insertion errors and retry logic

- [x] Add comprehensive error handling (AC: 2, 5)
  - [x] Implement exponential backoff for all API calls
  - [x] Use structured JSON logging with request_id
  - [x] Create error recovery strategies for each component
  - [x] Generate detailed error reports for troubleshooting

- [x] Integrate monitoring and metrics (AC: 3)
  - [x] Use SystemMonitor for health checks
  - [x] Record pipeline metrics (processing time, record counts, error rates)
  - [x] Generate transformation summary reports
  - [x] Implement alerting for critical failures

- [x] Support execution modes (AC: 4)
  - [x] Development mode: smaller date ranges, verbose logging
  - [x] Production mode: full date ranges, optimized for performance
  - [x] Dry-run mode: validation without data insertion
  - [x] Configuration-driven execution parameters

## Dev Notes

### Pipeline Architecture
**Source:** [architecture/components.md#orchestration]
The orchestrator coordinates these components:
- **Ingestion:** cursor_client, anthropic_client, sheets_connector
- **Processing:** validator, attribution, transformer
- **Storage:** bigquery_client with batch optimization

### Error Handling Strategy
**Source:** [architecture/error-handling-strategy.md]
- **Exponential backoff:** 2^attempt with max 3 retries
- **Partial success:** Continue pipeline if one API fails
- **Structured errors:** PipelineError dataclass with error_code, message, component
- **Recovery:** Log and continue with valid data, alert on critical failures

### Monitoring Integration
**Source:** [architecture/monitoring-and-observability.md]
- **Health Checks:** Use existing SystemMonitor and HealthChecker
- **Metrics:** Record processing_time_seconds, attribution_rate, error_rate
- **Alerting:** Integrate with AlertManager for critical failures
- **Logging:** Structured JSON with request_id for traceability

### Configuration Management
**Source:** [architecture/tech-stack.md#secret-management]
- **Secrets:** Use config.get_secret() for all API keys
- **Environment:** Support development/staging/production datasets
- **Batch Size:** 1000 records per BigQuery insert for cost optimization

### File Locations
**Source:** [architecture/unified-project-structure.md#src-organization]
- **Main file:** `src/orchestration/daily_job.py`
- **Entry point:** `src/main.py` (CLI interface)
- **Tests:** `tests/integration/test_pipeline_orchestrator.py`

## Testing

### Testing Standards
**Source:** [architecture/testing-strategy.md]
- **Framework:** pytest 7.4+ with pytest-mock
- **Test Types:** Unit tests for orchestrator logic, Integration tests for full pipeline
- **Coverage:** Target 90%+ coverage for orchestration module

### Test Requirements
- Mock all external API calls for unit tests
- Create integration test with real API data (small date range)
- Test error scenarios (API failures, BigQuery errors, attribution issues)
- Validate metrics collection and health check integration

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| September 27, 2025 | 1.0 | Initial story creation for pipeline orchestrator | Bob (SM) |

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4 (claude-sonnet-4-20250514[1m])

### Debug Log References
- Created comprehensive pipeline orchestrator with 500+ lines of production-ready code
- Implemented all error handling, monitoring, and execution modes
- Added data insertion methods to BigQuerySchemaManager
- Created comprehensive integration test suite
- Fixed logging setup compatibility issues

### Completion Notes List
- ✅ All Acceptance Criteria met (AC 1-6)
- ✅ DailyJobOrchestrator coordinates all pipeline components
- ✅ Comprehensive error handling with structured PipelineError dataclass
- ✅ Metrics collection and health check integration
- ✅ Support for development, production, and dry-run execution modes
- ✅ Structured JSON logging with request_id traceability
- ✅ Partial failure handling - continues processing if one API fails
- ✅ CLI interface for pipeline execution and health checks

### File List
- Created: `src/orchestration/daily_job.py` - Main pipeline orchestrator (500+ lines)
- Created: `tests/integration/test_pipeline_orchestrator.py` - Comprehensive integration tests
- Modified: `src/storage/bigquery_client.py` - Added insert_usage_data and insert_cost_data methods

## QA Results

### Review Date: September 26, 2025

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

Excellent implementation of a comprehensive pipeline orchestrator. The codebase demonstrates high engineering standards with robust error handling, comprehensive monitoring, and well-structured component architecture. The 550+ lines of production-ready code follow established patterns and provide extensive observability.

### Refactoring Performed

- **File**: src/orchestration/daily_job.py
  - **Change**: Enhanced health check component descriptions and added timestamps
  - **Why**: Improved debugging and monitoring capabilities
  - **How**: Added descriptive labels and check timestamps for better operational visibility

- **File**: src/storage/bigquery_client.py
  - **Change**: Added empty list validation to insertion methods
  - **Why**: Prevents unnecessary BigQuery API calls and improves logging
  - **How**: Added early return with informative logging for empty record sets

### Compliance Check

- Coding Standards: ✓ Follows Python best practices with proper imports, error handling, and logging
- Project Structure: ✓ Correctly placed in src/orchestration/ with proper module organization
- Testing Strategy: ✓ Comprehensive integration tests covering success, failure, and edge cases
- All ACs Met: ✓ All 6 acceptance criteria fully implemented and tested

### Improvements Checklist

- [x] Enhanced health check error descriptions (src/orchestration/daily_job.py)
- [x] Added empty list validation to BigQuery insertion methods (src/storage/bigquery_client.py)
- [x] Improved error context in health check exception handling
- [ ] Consider adding circuit breaker pattern for external API calls (future enhancement)
- [ ] Consider adding retry exponential backoff visualization in logs (future enhancement)
- [ ] Add integration test for BigQuery batch error scenarios (future enhancement)

### Security Review

No security concerns identified. The implementation properly:
- Uses configuration-based secret management via config.get_secret()
- Implements structured error handling without exposing sensitive data
- Follows secure logging practices with request_id traceability
- Validates and sanitizes data before BigQuery insertion

### Performance Considerations

Excellent performance considerations implemented:
- Configurable batch sizes (1000 for production, 100 for development)
- Parallel data fetching from multiple APIs
- Efficient error collection without blocking pipeline flow
- Proper resource cleanup and monitoring integration

### Files Modified During Review

- src/orchestration/daily_job.py: Enhanced health check descriptions and timestamps
- src/storage/bigquery_client.py: Added empty list validation to insertion methods

### Gate Status

Gate: PASS → docs/qa/gates/2.3-create-pipeline-orchestrator.yml

### Recommended Status

✓ Ready for Done - All acceptance criteria met, comprehensive testing implemented, code quality excellent